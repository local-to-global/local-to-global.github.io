<html>
<head>

<script type="text/x-mathjax-config">
  MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}});
</script>
<script type="text/javascript" async
  src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_CHTML">
</script>

<link rel="stylesheet" type="text/css" href="style.css">



<title>Tensors in Physics and Engineering</title>
</head>
<body>
  <div class="title">
    <h1 id="titleheading">Tensors in Physics and Engineering</h1>
  </div>
  <hr id="titlebreak">

  <div class="post-content">
  <div class="post-date">11/21/2016</div>
    <p>
      The concept of a "Tensor" is used a lot in physics and engineering, but what they actually are is left pretty mysterious. Vague descriptions as generalization of scalars, vectors, matrices, or as "multidimensional arrays" are all inaccurate being rooted in truth just enough to get students to stop asking what tensors are, without actually taking the time to discuss them. I hope to clear that up. 
    </p>
    <p>
      This will be a description of what tensors are, in the context of how they are used in physics and engineering. The solid math behind them is more complex and not particularly helpful for someone without a very solid backing in rigorous linear and abstract algebra. In particular, we will use naive notions of what vectors are, sticking with the special case of column arrays. This description assumes that we have already selected a basis of the vector space and are working with it. We'll have to discuss what happens when we change our basis. I'll also write column vectors as $(1,2,3)^T$, since it's really annoying to actually write them vertically.
    </p>
    <h3>Row Vectors, Column Vectors and Dot Products</h3>
    <p>
      Tensors are not "generalizations of scalars, vectors, matrices" or "multidimensional arrays", instead they can most intuitively thought of as follows:
    </p>
    <div class="thm"><b>Tensors are glorified Dot Products</b></div>
    <p> 
      If we have a column vector $\mathbf{v}$ and a row vector $\mathbf{w}$, then we can use the traditional matrix multiplication of $\mathbf{w}\mathbf{v}$ to get out a number. For example, if $\mathbf{v}=(1,2,3)^T$ and $\mathbf{w}=(3,2,1)$ then
      $$
      \mathbf{w}\mathbf{v} = (3,2,1)(1,2,3)^T = 3\cdot 1 + 2\cdot 2 + 1\cdot 3 = 10
      $$
      This means that combining a row vector and column vector through matrix multiplication gives us a real number. And this process is "linear", meaning that it interacts well with scalar multiplication and vector addition. This allows us to  discuss the dot product of two column vectors or two row vectors. If $\mathbf{a},\mathbf{b}$ are both column vectors, then their dot product is defined through matrix multiplication as:
      $$
      \mathbf{a}\cdot\mathbf{b} = \mathbf{a}^T\mathbf{b}
      $$
      If $\mathbf{c}$ and $\mathbf{d}$ are row vectors, then their dot product is defined through matrix multiplication as
      $$
      \mathbf{c}\cdot \mathbf{d} = \mathbf{c}\mathbf{d}^T
      $$
      We then have three operations: The "Dot Product" of a row vector and column vector, the "Dot Product" of two column vectors and the "Dot Product" of two row vectors. These dot products are, respectively, examples of a $(1,1)$-Tensor, a $(2,0)$-Tensor and a $(0,2)$-Tensor. The important thing is that these are functions that take in column/row vectors and output numbers, in a nice and linear fashion. We'll then work on generalizing <i>this</i> idea to understand what tensors are. 
    </p>
    <h3>Vectors, Linear Transformations and Tensors</h3>
    <p>
      If tensors are glorified dot products, then why do people relate them to vectors and matricies? People make this false equivalency because we can obtain tensors <i>from</i> vectors and linear transformations. But there is a big difference between saying that tensors generalize vectors, matrices etc, without mentioning their actual definitional nature, and providing you with a decent intuitive definition and illustrating how to obtain tensors from these objects
    </p>
    <p>
      Our mantra is that tensors are glorified dot products. That is, functions that take in column/row vectors and output numbers, following certain restrictions. Where dot products only input two vectors, we can have tensors input any number of vectors. With this in mind, is someone gives us a row vector, then we can create a $(1,0)$-Tensor from it. That is, if $\mathbf{c}$ is a row vector, we can create the tensor $T_{\mathbf{c}}(\mathbf{a})$, where $\mathbf{a}$ is a column vector, defined through matrix multiplication as
      $$
      T_{\mathbf{c}}(\mathbf{a}) = \mathbf{c}\mathbf{a}
      $$
      So we can create a tensor from a row vector. This is an example of a $(1,0)$-Tensor, because the input is one column vector. In a similar way, if $\mathbf{d}$ is a column vector, then we can create a $(0,1)$-Tensor from $\mathbf{d}$ in a similar way. We define $T_{\mathbf{d}}$ to be a function that inputs a row vector $\mathbf{b}$ defined as follows:
      $$
      T_{\mathbf{d}}(\mathbf{b}) = \mathbf{b}\mathbf{d}
      $$
      This is a $(0,1)$-Tensor, because it inputs one row vector and behaves like a dot-product. So we see that vectors are not tensors themselves, but ingredients to create tensors. 
    </p>
    <p>
      We can similarly look at matricies as ingredients for tensors. If $M$ is a square matrix, then we can create a $(1,1)$-Tensor from this. If $\mathbf{a}$ is a row vector and $\mathbf{c}$ is a column vector, then we can create the tensor $T_M(\mathbf{a},\mathbf{c})$ defined as:
      $$
      T_M(\mathbf{a},\mathbf{c}) = \mathbf{a}M\mathbf{c}
      $$
      This is a function that inputs one row vector, one column vector and outputs a real number in a way that behaves like a dot-procut. Note that if $M$ is the identity matrix, then this actually is the dot product of $\mathbf{a}$ and $\mathbf{c}$. Using transposes and the rank-1 tensors, you can mix and match all of these to create a wide variety of tensors, all of rank-1 or 2. But the key thing that all these have in common is not that they are arrays of numbers or anything, but that they are all functions that input vectors and output numbers in a way that behaves like the dot product. It should be noted that there are other ways to generalize dot products, namely inner products, and these can be seen as tensors. But inner products ensure to retain the geometric significance of the dot product, and can be viewed as tensors, but not all tensors are inner products.</p>
    <h3>General Tensors</h3>
    <p>
      We have already seen what rank-1 and rank-2 tensors look like. They are functions that input one or two vectors (row or column) and output numbers while behaving like dot products. How do we generalize this to an arbitrary number of vectors?
    </p>
    <p>
      Let $\mathbf{a}_1,...,\mathbf{a}_n$ be $n$-row vectors and $\mathbf{c}_1,...,\mathbf{c}_m$ be $m$-column vectors. We wish to describe what a rank $(m,n)$-Tensor is using these. A tensor of this rank will be a function of the form 
      $$
      T(\mathbf{a}_1,...,\mathbf{a}_n,\mathbf{c}_1,...,\mathbf{c}_m)
      $$
      The only thing we need to discuss is what it means to be "dot-product-like". The only thing that we need to enforce is that this function be linear in each variable. If $\mathbf{b}$ is a row vector, $\mathbf{d}$ is a column vector and $r$ is a real number, then we just need to have
      $$
      \begin{eqnarray*}
      T(\mathbf{a}_1,...,\mathbf{b}+r\mathbf{a}_i...,\mathbf{a}_n,\mathbf{c}_1,...,\mathbf{c}_m) &=& T(\mathbf{a}_1,...,\mathbf{b},...,\mathbf{a}_n,\mathbf{c}_1,...,\mathbf{c}_m)\\&& + r T(\mathbf{a}_1,...,\mathbf{a}_i...,\mathbf{a}_n,\mathbf{c}_1,...,\mathbf{c}_m)\\
      T(\mathbf{a}_1,...,\mathbf{a}_n,\mathbf{c}_1,...,\mathbf{d}+r\mathbf{c}_j...,\mathbf{c}_m) &=& T(\mathbf{a}_1,...,\mathbf{a}_n,\mathbf{c}_1,...,\mathbf{d},...,\mathbf{c}_m)\\&& + rT(\mathbf{a}_1,...,\mathbf{a}_n,\mathbf{c}_1,...,\mathbf{c}_j,...,\mathbf{c}_m)
      \end{eqnarray*}
      $$
      And this is true regardless of where the sums happen. This is what it means for a function to be linear in each term. A $(n,m)$-Tensor is then a function that inputs $n$ row vectors and $m$ column vectors that is linear in every term. This is all that tensors are. 
    </p>
    <p>
      Now, this condition is pretty abstract. It was helpful in the rank 1 and rank 2 cases to be able to use vectors and matricies to write down and explicitly compute tensors. This can, indeed, be done in the more general case. To be able to express these, it is helpful to explicitly write down a formula for the $(1,1)$-Tensor $T_M(\mathbf{a},\mathbf{c})$ created in the previous section. We say that $\mathbf{a}=(a^1,...,a^k)$, $\mathbf{b}=(b_1,...,b_k)^T$ and that the $i,j$th entry of $M$ is $M^i_j$. We can then write a formula for $T_M(\mathbf{a},\mathbf{c})$:
      $$
      T_M(\mathbf{a},\mathbf{c}) = \sum_{i,j=1}^k M^i_ja^jc_i
      $$
      The superscripts and subscripts are there to keep track of how elements interact with each other. This kind of formula pops up a lot in all areas of math, because in forces a function to be linear. Particularly, we can express and compute arbitrary tensors using a similar formula. We have:
      $$
       T(\mathbf{a}_1,...,\mathbf{a}_n,\mathbf{c}_1,...,\mathbf{c}_m) = \sum_{i_1,...,i_n,j_1,...,j_m = 0}^{k} T_{j_1,...,j_n}^{i_1,...,i_m} a_1^{(i_1)}\cdots a_n^{(i_n)}c_{1,(j_1)}\cdots c_{m,(j_m)}
      $$
      This formula is kinda a mess, so if you are working in generality it it impractical, but if you have a particular tensor of small rank, then it can be really helpful in computing things. To compute with this formula, we have a huge $m+n$-dimensional array of numbers $T_{j_1,...,j_n}^{i_1,...,i_m}$ that we are using to compute this tensor. This is how "multidimensional arrays" get confused with tensors. We can use multidimensional arrays of numbers to construct and compute tensors, but they are, again, ingredients that are not needed for tensors, but are useful when talking about them.
    </p>
    <p>
      Confusing tensors with matricies, or other multidimensional arrays of numbers, is the same thing as confusing a formula with a function. The expression $3x+1$ is not a function, but you can use it to create a function defined by $f(x)=3x+1$. The array $M^i_j$ is not a tensor, but you can use it to create the function $T_M$. 
    </p>
    <h3>Tensors are Functions</h3>
    <p>
      To drive home the fact that tensors are functions and not generalizations of vectors or multidimensional arrays, we investigate necessarily functional properties of them. Particularly, you often hear from physicists that tensors are "things that transform like tensors", which is unhelpful without context. But we have context now. If we have a matrix $M$ and create the tensor $T_M$ using it, then our formula for evaluating it, $T_M(\mathbf{a},\mathbf{c}) = \sum_{i,j=1}^k M^i_ja^jc_i$ depends on the basis that we are using for our vector spaces. If we change the basis, then the arrays used to represent $M$, $\mathbf{a}$ and $\mathbf{c}$ will change, but the actual vectors they represent and the actual linear transformation that $M$ represents will not change. And since $T_M$ is a function on vectors, not a function on arrays of numbers, then it follows that the value of $T_M(\mathbf{a},\mathbf{c}$ can not depend on how we decide to write down these components using bases and arrays. If we change our basis, then the value of $T_M(\mathbf{a},\mathbf{c})$ must remain the same. The formula for "transforming like a tensor" is then just the change-of-basis formula for multidimensional arrays. 
    </p>
    <p>
      But the important thing is that multiple arrays can represent the same tensor. Furthermore, depending on the basis, a single array can represent different tensors. They key thing about tensors is that they are actually functions, and it is sometimes convenient to use arrays to compute them. These two things are often confused. 
    </p>
    <p>
      If, for instance, we have a function where you input the height a ball is dropped, and it outputs the speed of the ball after some time $t$. If we use standard units, then this function is given by $d(t) = -9.8t$. But if we decide to use feet instead of meters, then this becomes $d(t)=-32.2t$. This is the same function, but changing how we express the quantities changes how the function looks. The important thing is the function $d(t)$, not the coefficient out front, because it depends on the units we use. Same thing for tensors. Multidimensional arrays are glorified coefficients and they depend on the coordinates we choose, and  so the important thing is the actual function rather than the arrays.
    </p>
      
    <h3>A Note on Tensor Fields</h3>
    <p>
      What has been described is a purely algebraic look at tensors. These kinds of tensors are used a lot in engineering, as well as in physics. But in more sophisticated theories in physics, this concept of a tensor is expanded to "Tensor Fields" and a lot of the algebraic stuff is taken care of by the geometry.
    </p>
    <p>
      For tensor fields, we start with an geometric object, perhaps a sphere, a cylinder, a torus, flat 3D space or even 4D spacetime. At every point on such objects, we can create a Tangent Space. The Tangent Space at a point is a vector space, with the same dimension as the object in question, and encodes how derivatives can behave at that point. Particularly, the elements of the tangent space are all possible directional derivatives at that point. There is also a twin space called the Cotangent Space that is also a vector space, but is the vector space of all possible "total derivatives" at that point. 
    </p>
    <p>
      If the object has coordinates, then you can get an explicit basis for both the tangent space and cotangent space for free from the coordinates. If you change the object's coordinates, then you change the basis of tangent space and cotangent space, and this change of basis is mediated by the Jacobian matrix.
    </p>
    <p>
      As an example, 3D Euclidean space has coordinates $(x,y,z)$ and if $P$ is a point in $3D$ space with tangent space $T\mathbb{R}^3_P$, then the elements of this tangent space look like
      $$
      \partial_v = v_x\frac{\partial}{\partial x}\big|_P + v_y\frac{\partial}{\partial y}\big|_P + v_z\frac{\partial}{\partial z}\big|_P
      $$
      If $f(x,y,z)$ is a function differentiable at the point $P$, then $\partial_v(f)$ isthe directional derivative in the direction of $\mathbf{v}=(v_x,v_y,v_z)^T$ evaluated at $P$. We can then view the tangent space as column vectors. On the other hand, an element of the cotantent space $T^*\mathbb{R}^3_P$ looks like
      $$
      dw = w_xdx_P + w_ydy_P + w_zdz_P
      $$
      If $f(x,y,z)$ is differentiable at $P$ then we get the total derivative $df=f_x(P)dx+f_y(P)dy+f_z(P)dz$, which is an element of the cotangent space at $P$. We can then view elements of the cotangent space as row vectors $(w_x,w_y,w_z)$ and, in particular, $df=(f_x(P),f_y(P),f_z(P))$. Note that the directional dereivative of $f(x,y,z)$ in the direction of $\mathbf v$ is just the dot product $df\cdot \mathbf{v}$. 
    </p>
    <p>
      A vector field on an object is then a choice of a tangent vector in every tangent space, ie a tangent vector at every point. This choice of vector must "vary smoothly", which is a differentiability condition on these vectors. Similarly a covector field is a choice of covector at every point that varies smoothly. A Tensor Field follows this reasoning. It is a choice of tensor that inputs tangent and cotangent vectors at every point that varies smoothly. If you change coordinates of your object, then the coordinates of the array representing the tensor must change based on what the Jacobian matrix says. This is where the traditional formula for "A tensor is something that transforms like a tensor" comes from. 
    </p>
    <p>
      When they talk about tensors in general relativity, they are referring to these tensor fields. Changing coordinates of the object then changes all the bases in all the tangent/cotangent spaces, forcing the arrays to represent the tensor at each point to change accordingly.
    </p>
    <h3>Conclusion</h3>
    <p>
      The best one-liner for what a tensor is is as follows:
    <div class="thm"><b>A Tensor is a generalization of the Dot Product</b></div>
    All of this, however, is just a taste of what is behind tensors. A lot is taken for granted and there is a lot of subtle linear algebra and differntial geometry behind all of this. The connection between raising and lowering indicies, transposition, dual spaces and inner products is vitally important to applications. Differentiability conditions on tensor fields, differential forms, and more are all key to things in general relativity. Abstract tensor products are not even mentioned here, but they are related to these as well. The goal of this is to provide a satisfying responce and subsequent explanation for those who inquire about the nature of tensors. Which is something absent from all literature about the topic in the undergraduate level.
    </p>
      
      
      
   
      

  </div>



</body>
</html>
