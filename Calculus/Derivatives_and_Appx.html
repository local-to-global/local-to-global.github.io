<html>
<head>

<script type="text/x-mathjax-config">
  MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}});
</script>
<script type="text/javascript" async
  src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_CHTML">
</script>

<link rel="stylesheet" type="text/css" href="style.css">



<title>Taylor Polynomials and Derivatives</title>
</head>
<body>
  <div class="title">
    <h1 id="titleheading"><a href="index.html" class="title-link">Local-to-Global</a></h1>
  </div>
  <hr id="titlebreak">
  
  <h2 id = "titleheading">Taylor Polynomials and Derivatives</h2>
  <div class="post-content">
  <div class="post-date">5/31/2017</div>
  <h3>Tangent Lines and Derivatives</h3>
  <p>
  The heard of Calculus is Approximation, and the objects of Calculus are Functions. This means that Calculus can be thought of as the study of functions using tools of approximation. One particular strategy for this kind of study are methods to approximate functions with simple functions, particularly polynomials. It turns out, that when we try to understand functions through this lens, derivatives pop out as natural objects of interest.
  </p>
  <h4> Functions are Hard</h4>
  <p>
  There are only a few functions that we can write down and work with. These are lines, quadratics, and higher degree polynomials, exponential functions, logarithmic functions, trigonometric functions, hyperbolic functions, and a splattering of other functions like the absolute value. This is, however, a small collection of functions and while we can combine and compose these functions, almost every function is still out of our reach. Even the functions that we're interested in cannot be constructed from the familliar functions that we can easily work with.
  <p>
  This is a bit of a problem because we use functions for everything. Predicting sales of a product. Encoding valuable information through hash functions. The AI control of devices. Machine Learning algorithms. COVID-19 data curves. The Stock Market. The various aspects of Climate Change. All of these are uses of functions for very important things, and we can write down none of these functions. We can sometimes write down rules that these functions have to obey, though even this can be quite inaccessible, but this doesn't mean we can directly access the functions themselves. They're too complex and too hard.
  </p>
  <p>
  Regardless, however, this doesn't mean that we can't say anything meaningful about these functions. One way to study a hard function is to take a class of functions that we do know, like polynomials, and to approximate the function in question using them. If we can find the cubic polynomial which best approximates a function of interest, then we can use this cubic, an easy function, as a stand-in for the hard function (as long as we're careful). This is the idea behind a lot of analysis found in Calculus.
</p>

<h4>Tangent Lines</h4>
  <p>
  Lines are particularly simple to work with, and most functions look like straight lines when you zoom in enough, so it can be illuminating to try and figure out which line approximates a function best around a certain point. If $f(x)$ is a function, and $(a,f(a))$ is the point that we're interested in, then we want to find the line that looks like $f(x)$ near $(a,f(a))$ the most. Obviously, this line will have to pass through $(a,f(a))$ and so it must look like
  $$
  y = f(a) + m(x-a)
  $$
  These lines, if we vary $m$, will give us all the non-vertical lines which pass through the point $(a,f(a))$. It's just a question of finding the one that fits the function the best. What this means is that, if $x$ is close enough to $a$ then corresponding point on the line and the value $f(x)$ will be approximately equal and, moreover, this approximation gets better and better the closer we get to $x=a$. That is, for $x$ close enough to $a$, we can write
  $$
  f(x) \approx f(a) + m(x-a)
  $$
  We can actually use this to get an idea of what $m$ has to be in order to get a good approximation. Using this expression to approximately solve for $m$ gives:
  $$
  m \approx \frac{f(x)-f(a)}{x-a}
  $$
  Now, of course, given any fixed $x$ close to $a$, there could be many different $m$s which provide good-enough approximations. but the closer $x$ is to $a$, the less leway we'll have. So if we want to find the absolute best $m$, then we should take $x=a$ in this formula. Unfortunately, that gives $0/0$, which is problematic. We can, instead, take the limit as $x\rightarrow a$. This also functions to exclude the less-best $m$s, leaving only the optimal one. We then get
  $$
  m = \lim_{x\rightarrow a} \frac{f(x)-f(a)}{x-a}
  $$
  This gives us the most optimal $m$, which we usually write $f'(a)$, and so it also gives us the most optimal line:
  $$
  T_a(x) = f(a)+f'(x)(x-a)
  $$
  This is called the Tangent Line of $f(x)$ at $x=a$. This tangent line then gives us access to some of the more complicated information contained in $f(x)$. We often write this approximation as
    $$
    f(a+\Delta x) \approx f(a) + f'(a)\Delta x
    $$
    This approximation has intuitive meaning as well. A line increases at a steady rate, as determines by its slope. A function, however, can turn and change its direction all the time and so it does not actually make sense to talk about the slope of an arbitrary function. But if we can approximate this function using a line then if we had to talk about the slope of the function we could say that it is the slope of the approximating line. This is the best possible answer for a question which has no answer. This allows us to use our understanding of slope to make inferences about the function itself.

  </p>
  <h3> Tangent Quadratics</h3>
  <p>
    Using lines to approximate functions is good and all, but a line can only pull so much weight. If we want finer data about our function we'll have to do approximations using functions with a bit more clout. The natural progression is to look at approximating quadratics. Any quadratic passing through $(a,f(a))$ will look like
    $$
    q(x) = r(x-a)^2+s(x-a)+f(a)
    $$
    for some $r$ and $s$. We want to find the optimal $r,s$ so that when $x$ is close enough to $a$ we can write 
    $$
    f(x)\approx r(x-a)^2+s(x-a)+f(a)
    $$
    In this situation, there are two parameters to work with and so it might be a little tougher to find the fit. But trick is to notice that for $x$ close enough to $a$, the quadratic term is super small, much smaller than the linear term and so we can ignore it. This means for $x$ really close to $a$ we must have
    $$
    f(x) \approx s(x-a)+f(a)
    $$
    But we already know what $s$ is in this case, $s=f'(a)$. We can plug this in and see that we just have to find the optimal $r$ to fit the approximation
    $$
    f(x) \approx r(x-a)^2 +f'(a)(x-a)+f(a)
    $$
    Reasoning in the same way as above, we get that
    $$
    r = \lim_{x\too a} \frac{f(x)-f(a)-f'(a)(x-a)}{(x-a)^2}
    $$
    This may not be the most familiar formula, but it is actually a way to compute half the second derivative of a function. We sometimes write $r=f''(a)/2$. This helps us figure out the most optimally approximating quadratic. This approximation is finer than the approximation given by a line, but it is still relatively simple enough to be useful in applications. This has an intuitive reading as well. In general, a function is not a quadratic, but if we had to say that it was quadratic near a point, then this would be the best possible answer. We can then see quadratic features of a functions through this approximation, features such as maximum and minimum points.
    </p>
    <h3>Tangent Polynomials</h3>
    <p>
      This line of reasoning can continue. For our difficult function $f(x)$, we can try to find the best approximating polynomial of each degree. Each iteration when we try to do this, we rediscover higher order derivatives. The general formula for the best approximating degree $n$ polynomial for $f(x)$ around $x=a$ is
      $$
      f(x) \approx f(a) + f'(a)(x-a) + f''(a)\frac{(x-a)^2}{2!} + f'''(a) \frac{(x-a)^3}{3!} +\cdots +f^{(n)}(a)\frac{(x-a)^n}{n!}
      $$
      In fact, we can take the coefficients of these polynomials to be the definitions of these derivatives: The $n$th order derivative of $f(x)$ is $n!$ times the coefficient of $(x-a)^n$ in the best approximating polynomial of degree at least $n$. Now, each of these approximating polynomials can give us information about the kinds of features that appear in polynomials of various degrees. We can get information about inflection, or higher order maximums and minimums. All of this is done through approximation.
    </p>
    <h3>Taylor Series</h3>
    <p>
      The idea can be pushed even further. In general, as you increase the degree of the polynomial the better the approximation will be. And so that can lead us to ask what happens if we have an "infinite degree polynomial" (also known as a power series)? It turns out that, for nice functions, this approximation does get better and better. So good, in fact, that the function becomes equal to the power series. That is for nice functions we have:
      $$
      f(x) = \frac_{n=0}^{\infty} f^{(n)}(a)\frac{(x-a)^n}{n!}
      $$
      This is called the Taylor Series for $f(x)$ at $x=a$. But there is a balance to be struck. Approximations get worse as you stray from the central point, and how this manifests in the infinite degree polynomial is that it will diverge after some point. There is some distance you can stray from the point of interest where the power series equals the function. But if you stray too far the lack of approximation takes over and the series violently becomes not equal to the function due to a sudden lack of convergence. So for these functions, we can find patches where the function equals some nice power series.
    </p>
    <p>
      It should be noted, however, that this only works for very nice functions. The function has to look like a polynomial enough to be approximated by polynomials. There are functions which look nothing like a polynomial, even on small scales. There are functions that look like any degree polynomial we want, but they don't approximate strongly enough to produce a Taylor series. And so even if we have opened the door to more complicated functions, we've only barely cracked the door. Is there a way to understand even more complex families of functions?
    </p>
    
  </div>



</body>
</html>
