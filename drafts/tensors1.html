<html>
<head>

<script type="text/x-mathjax-config">
  MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}});
</script>
<script type="text/javascript" async
  src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_CHTML">
</script>

<link rel="stylesheet" type="text/css" href="style.css">



<title>What are Algebraic Tensors?</title>
</head>
<body>
  <div class="title">
    <h1 id="titleheading">What are Algebraic Tensors?</h1>
  </div>
  <hr id="titlebreak">

  <div class="post-content">
  <div class="post-date">11/21/2016</div>
    <p>
      If you have ever sat in a math, physics or engineering course, working through manipulations and ask yourself "What the heck are Tensors?!" then this is the place for you. If you've then asked someone and they said "They're generalizations oof scalars, vectors and matrices" or "They're multidimensional arrays" or "They're things that transform like Tensors", and felt that these were inadequet explanations, just left more confused or left you with more questions/curiosity, then this is the place for you. 
    </p>
    <p>
      Tensors are really powerful objects that can encode a lot of information. Physical as well as geometric information. The issue is that they pop up in different contexts, and what is meant by a "tensor" depends on that context. I'll try to go over all of these different contexts. They are each related, but function differently from each other and so a single answer to "What is a Tensor?" does not work for all of them (and usually doesn't work for any of them). Because they are so powerful, but their definition is never made clear and is relatively more abstract than the typical toolbox for a physicist or engineer (and because mathematicians usually don't encounter the same tensors that physcists/engineers do), what we're left with is a bunch of people using tensors without actually knowing what the heck they are! I hope to  set this  straight. The price of setting it straight, however, is working at a higher level of  abstraction than you might encounter in a typical physics or engineering degree. It's not harder, you just might have to get out of your comfort zone a bit.
    </p>
    <p>
      In this bit, we're going to look at the simplest kind of tensor: Algebraic Tensors. These are probably the most common tensor that is encountered in everyday work for engineers and physicists (unless you do General Relativity). Common examples of tensors of this type are the Cauchy-Stress Tensor and the Inertial Tensor. As a plan of attack, we will first get you out of your comfort zone a bit by transitioning from vectors as "things with magnitude and direction" or "arrays of numbers" to more abstract notions of vectors. This is an unfortunate prerequisite to understanding tensors. You can work with tensors without making this leap, but you can't understand them without it. Along the way we'll introduce covectors and Dual Spaces. Next we'll cover rank-1 tensors, and investigate their connection to vectors and covectors. After that, we'll introduce tensors of general rank. At this point, we'll know the nature of what tensors are, but be unable to do computations of any kind, as we'll be lacking the tools needed to compute with tensors, so we'll introduce those tools next and see how tensors are often confused with arrays of numbers. Finally, we'll look at the examples of the Cause-Stress Tensor and the Inertial Tensor as explicit examples of tensors, working from what these tensors do abstractly as tensors and how we compute/work with them.
    </p>
    <h3>Vectors and Covectors</h3>
    <p>
      Vectors are not lists of numbers. Vectors are not things with direction and magnitude. Vectors are elements of a vector space. Sometimes we can represent and compute vectors using lists of numbers. Working with them in this way makes them fool-proof, you don't need to know much about vectors to begin working with them when you're only given them as arrays of numbers. These are the "Apple Product" version of a vector, anyone can use them without really knowing what they're doing. Also, in really, really nice vector spaces we can talk about notions like "direction" and "magniture", but these are extra addons not typically included when you have a vector. We're working with the basics, not the premium package. 
    </p>
    <p> A vector is an element of a vector space. A vector space $V$ is a collection of things that we can add together and scale by a real (or complex) number. "Scale" is a generous term, a better way to say it is that a vector space is a collection things that we can add together and multiply by real or complex numbers. This means that if $v$ and $w$ are in the vector space $V$, then there is some notion of "Addition" so that $v+w$ is also in $V$, additionally if $r$ is a real (or complex) number then there is a notion of "Multiplication" so that $r\cdot v$ is in $V$. What these addition and multiplication operations are depends on the vector space in question, but they obey the kinds of equations that you would expect them to, like distributivity and associativity. There's also a zero vector that adds just like zero usually does, and multiplication by 1 does what typical multiplication by 1 does: Nothing. The important thing is that there are these nice rules that addition/multiplication in a vector space obey, but there is no formula for addition or multiplication in a vector space. How addition and multiplication are actually computed depends on the vector space you have, so they aren't important to what a vector space actually *is*.
    </p>
    <p>
      If $V$ is a vector space, then an element $v$ inside it is called a "vector". This is the truest notion of a vector.
    </p>
    <p>
      As an example, the set of functions that are differentiable on the entire real line forms a vector space. If $f(x)$ and $g(x)$ are two differentiable functions, then we can add them together to get a new function $f+g$ defined as $(f+g)(x)=f(x)+g(x)$. We can also  multiply $f$ by a real number $r$ to get a new differentiable function $rf$ defined as $(rf)(x) = r\cdot f(x)$. These notions of "Addition" and "Multiplication" work like we would expect them to, and so we get a vector space. Note that diffentiable functions are not arrays of numbers, and that there is no notion of "direction" or "magnitude" for an arbitrary differentiable function, nonetheless a differentiable function $f(x)$ is a vector, since it is an element of a vector space. This vector space can be difficult to understand, as it is infinite dimensional and that is a whole other monster, but it serves as a nice reference point to think back on when you catch yourself thinking of vectors as arrays of numbers or as things with direction and magnitude. In general, function spaces (vector spaces of functions) are some of the most important vector spaces in all of math.
    </p>
    <p>
      If $V$ is a vector space, then we can create a new vector space from it for free. This can be thought of as the "Mirror Version" of the vector space in question, and is called the "Dual Space", denoted $V^*$. The vector space $V^*$ is a function space, which  means its elements are functions. In particular, an element $\alpha$ of $V^*$ is a function from $V$ to the real numbers (or complex numbers) that "respect" vector addition and scalar multiplication. Explicitly, if $v,w$ are vectors in $V$ and $r$ is a real (or complex) number, then the element $\alpha$ in $V^*$ is a function from $V$ that outpus real (or complex) numbers and satisfies
      $$
      \begin{eqnarray*}
      \alpha(v+w) &=& \alpha(v) + \alpha(w)\\
      \alpha(r\cdot v) &=& r\cdot\alpha(v)
      \end{eqnarray*}
      $$
    The set of all functions that satisfy these equations forms a vector space, and we call it $V^*$, the dual space of $V$. These vectors are sometimes called "Linear Functionals", because they work a lot like basic linear functions from high school algebra, but we're going to call them "Covectors". While a covector $\alpha$ is actually a vector, since it is an element of the vector space $V^(*)$, calling it a covector emphasizes its relationship to $V$ as a function.
    </p>
    <p>
      As a quick example, we can find a pretty nice covector for the vector space of differentiable functions. This will have to be a function that inputs differentiable functionos and outputs a real number. We can make the covector $ev_0$ which, when input the function $f(x)$, outputs the number $f(0)$. So $ev_0(f)=f(0)$. It is not too difficult to check that this satisfies the requisites for a covector, so it is an element of the dual space of differentiable functions. In fact, for any real number $r$, we can get the covector $ev_r(f)=f(r)$. The function $ev_r$ is called the "evaluation at $r$" function. We input different functions and see what value they output at this particular point. Similarly, we can make $dev_r(f)=f'(r)$, which outputs the value of the derivative at $r$. This is also a covector. While dual spaces for this infinite dimensional space doesn't work as nicely as in the finite dimensional case, these are great examples of what covectors can be.
    </p>
      
      
      
      
      

  </div>



</body>
</html>
