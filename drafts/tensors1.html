<html>
<head>

<script type="text/x-mathjax-config">
  MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}});
</script>
<script type="text/javascript" async
  src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_CHTML">
</script>

<link rel="stylesheet" type="text/css" href="style.css">



<title>What are Algebraic Tensors?</title>
</head>
<body>
  <div class="title">
    <h1 id="titleheading">What are Algebraic Tensors?</h1>
  </div>
  <hr id="titlebreak">

  <div class="post-content">
  <div class="post-date">11/21/2016</div>
    <p>
      If you have ever sat in a math, physics or engineering course, working through manipulations and ask yourself "What the heck are Tensors?!" then this is the place for you. If you've then asked someone and they said "They're generalizations oof scalars, vectors and matrices" or "They're multidimensional arrays" or "They're things that transform like Tensors", and felt that these were inadequet explanations, just left more confused or left you with more questions/curiosity, then this is the place for you. 
    </p>
    <p>
      Tensors are really powerful objects that can encode a lot of information. Physical as well as geometric information. The issue is that they pop up in different contexts, and what is meant by a "tensor" depends on that context. I'll try to go over all of these different contexts. They are each related, but function differently from each other and so a single answer to "What is a Tensor?" does not work for all of them (and usually doesn't work for any of them). Because they are so powerful, but their definition is never made clear and is relatively more abstract than the typical toolbox for a physicist or engineer (and because mathematicians usually don't encounter the same tensors that physcists/engineers do), what we're left with is a bunch of people using tensors without actually knowing what the heck they are! I hope to  set this  straight. The price of setting it straight, however, is working at a higher level of  abstraction than you might encounter in a typical physics or engineering degree. It's not harder, you just might have to get out of your comfort zone a bit.
    </p>
    <p>
      In this bit, we're going to look at the simplest kind of tensor: Algebraic Tensors. These are probably the most common tensor that is encountered in everyday work for engineers and physicists (unless you do General Relativity). Common examples of tensors of this type are the Cauchy-Stress Tensor and the Inertial Tensor. As a plan of attack, we will first get you out of your comfort zone a bit by transitioning from vectors as "things with magnitude and direction" or "arrays of numbers" to more abstract notions of vectors. This is an unfortunate prerequisite to understanding tensors. You can work with tensors without making this leap, but you can't understand them without it. Along the way we'll introduce covectors and Dual Spaces. Next we'll cover rank-1 tensors, and investigate their connection to vectors and covectors. After that, we'll introduce tensors of general rank. At this point, we'll know the nature of what tensors are, but be unable to do computations of any kind, as we'll be lacking the tools needed to compute with tensors, so we'll introduce those tools next and see how tensors are often confused with arrays of numbers. Finally, we'll look at the examples of the Cause-Stress Tensor and the Inertial Tensor as explicit examples of tensors, working from what these tensors do abstractly as tensors and how we compute/work with them.
    </p>
    <h3>Vectors and Covectors</h3>
    <p>
      Vectors are not lists of numbers. Vectors are not things with direction and magnitude. Vectors are elements of a vector space. Sometimes we can represent and compute vectors using lists of numbers. Working with them in this way makes them fool-proof, you don't need to know much about vectors to begin working with them when you're only given them as arrays of numbers. These are the "Apple Product" version of a vector, anyone can use them without really knowing what they're doing. Also, in really, really nice vector spaces we can talk about notions like "direction" and "magniture", but these are extra addons not typically included when you have a vector. We're working with the basics, not the premium package. 
    </p>
    <p> A vector is an element of a vector space. A vector space $V$ is a collection of things that we can add together and scale by a real (or complex) number. "Scale" is a generous term, a better way to say it is that a vector space is a collection things that we can add together and multiply by real or complex numbers. This means that if $v$ and $w$ are in the vector space $V$, then there is some notion of "Addition" so that $v+w$ is also in $V$, additionally if $r$ is a real (or complex) number then there is a notion of "Multiplication" so that $r\cdot v$ is in $V$. What these addition and multiplication operations are depends on the vector space in question, but they obey the kinds of equations that you would expect them to, like distributivity and associativity. There's also a zero vector that adds just like zero usually does, and multiplication by 1 does what typical multiplication by 1 does: Nothing. The important thing is that there are these nice rules that addition/multiplication in a vector space obey, but there is no formula for addition or multiplication in a vector space. How addition and multiplication are actually computed depends on the vector space you have, so they aren't important to what a vector space actually *is*.
    </p>
    <p>
      If $V$ is a vector space, then an element $v$ inside it is called a "vector". This is the truest notion of a vector.
    </p>
    <p>
      As an example, the set of functions that are differentiable on the entire real line forms a vector space. If $f(x)$ and $g(x)$ are two differentiable functions, then we can add them together to get a new function $f+g$ defined as $(f+g)(x)=f(x)+g(x)$. We can also  multiply $f$ by a real number $r$ to get a new differentiable function $rf$ defined as $(rf)(x) = r\cdot f(x)$. These notions of "Addition" and "Multiplication" work like we would expect them to, and so we get a vector space. Note that diffentiable functions are not arrays of numbers, they aren't "arrows" and that there is no notion of "direction" or "magnitude" for an arbitrary differentiable function, nonetheless a differentiable function $f(x)$ is a vector, since it is an element of a vector space.This vector space can be difficult to understand, as it is infinite dimensional and that is a whole other monster, but it serves as a nice reference point to think back on when you catch yourself thinking of vectors as arrays of numbers or as things with direction and magnitude.  Vectors are understood by how they interact with other vectors, and this can really help illustrate this. In general, function spaces (vector spaces of functions) are some of the most important vector spaces in all of math.
    </p>
    <p>
      If $V$ is a vector space, then we can create a new vector space from it for free. This can be thought of as the "Mirror Version" of the vector space in question, and is called the "Dual Space", denoted $V^*$. The vector space $V^*$ is a function space, which  means its elements are functions. In particular, an element $\alpha$ of $V^*$ is a function from $V$ to the real numbers (or complex numbers) that "respect" vector addition and scalar multiplication. Explicitly, if $v,w$ are vectors in $V$ and $r$ is a real (or complex) number, then the element $\alpha$ in $V^*$ is a function from $V$ that outpus real (or complex) numbers and satisfies
      $$
      \begin{eqnarray*}
      \alpha(v+w) &=& \alpha(v) + \alpha(w)\\
      \alpha(r\cdot v) &=& r\cdot\alpha(v)
      \end{eqnarray*}
      $$
    The set of all functions that satisfy these equations forms a vector space, and we call it $V^*$, the dual space of $V$. These vectors are sometimes called "Linear Functionals", because they work a lot like basic linear functions from high school algebra, but we're going to call them "Covectors". While a covector $\alpha$ is actually a vector, since it is an element of the vector space $V^*$, calling it a covector emphasizes its relationship to $V$ as a function.
    </p>
    <p>
      As a quick example, we can find a pretty nice covector for the vector space of differentiable functions. This will have to be a function that inputs differentiable functionos and outputs a real number. We can make the covector $ev_0$ which, when input the function $f(x)$, outputs the number $f(0)$. So $ev_0(f)=f(0)$. It is not too difficult to check that this satisfies the requisites for a covector, so it is an element of the dual space of differentiable functions. In fact, for any real number $r$, we can get the covector $ev_r(f)=f(r)$. The function $ev_r$ is called the "evaluation at $r$" function. We input different functions and see what value they output at this particular point. Similarly, we can make $dev_r(f)=f'(r)$, which outputs the value of the derivative at $r$. This is also a covector. While dual spaces for this infinite dimensional space doesn't work as nicely as in the finite dimensional case, these are great examples of what covectors can be.
    </p>
    <p>
      To summarize, we write:
    <div class="thm"><b>Definition: </b> Let $V$ be vector space over some field. Then
      <ul>
        <li>A <b>Vector</b> of $V$ is any element of $V$</li>
        <li> A <b>Covector</b> of $V$ is any element of $V^*$. That is, a linear function $\alpha(v)$ from $V$ into the base field.</li>
      </ul>
    </div>
      </p>
    <h3> Rank-1 Tensors</h3>
    <p>
      Now that we know what vectors and covectors are, and we are free from thinking of vectors as arrows, or arrays or something, we can start talking about tensors.
    </p>
    <p> A $(0,1)$-Tensor on the vector space $V$ is a function $T$ that inputs vectors and outputs real (or complex) numbers satisfying the equations
      $$
      \begin{eqnarray*}
      T(v+w) &=& T(v) + T(w)\\
      T(r\cdot v) &=& r\cdot T(v)
      \end{eqnarray*}
      $$
      for vectors $v,w$ in $V$ and scalar $r$. In other words, a $(1,0)$-Tensor is a covector. This should be seen as the prototypical example of a tensor, and general tensors will then be a generalization of covectors rather than a generalization of vectors. What's happening is that we find that covectors and dual spaces are great at understanding vector spaces, so we want to generalize them to be more descriptive, and the key things that we will be generalizing are these linearity relations and the fact that covectors are necessarily functions.
    </p>
    <p>The first way to generalize this is to change the type of input from vectors to covectors themselves. A $(1,0)$-Tensor on the vector space $V$ is then a function $f$ that inputs covectors and outputs real (or complex) numbers so that 
      $$
      \begin{eqnarray*}
      f(\alpha+\beta) &=& f(\alpha) + f(\beta)\\
      f(r\cdot \alpha) &=& r\cdot f(\alpha)
      \end{eqnarray*}
      $$
      for covectors $\alpha,\beta$ in $V^*$ and scalar $r$. In other words, such a  tensor is a cocovector, or a covector for the vector space $V^*$. The really magical thing is that each of these tensors can be represented uniquely as a vector in $V$. What this means is that we can interpret each vector $v$ in $V$ as a $(1,0)$-tensor, or as a cocovector. This is typically expressed as $V^{**}\simeq V$, which means that the dual space of a dual space is, essentially, just the original vector space (the dual space is like a "mirror" space for a reason). Concretely, if $v$ is a vector in $V$, then we can make the $(1,0)$-Tensor $f_v$ defined as
      $$
      f_v(\alpha)=\alpha(v)
      $$
      In other words, this is the evaluation function at the point $v$. The magic is that in finite dimensional spaces (and really, really nice infinite dimensional spaces) every one of these cocovectors has this description. This isn't true in general.
    </p>
    <p>So every vector $v$ in $V$ can be interpreted as a tensor, but this description is not fundamental to being a vector since vectors are no a priori functions, but tensors are. This means that tensors are not generalizations of a vector, instead they are generalization of covectors, which are functions, a priori. It is just a really nice and convenience that vectors can be interpreted as tensors as well. The corrected statement to "Tensors generalize vectors" is then "Tensors generalize covectors".
    </p>
    <p>
      To summarize, $(0,1)$-Tensors are covectors, linear functions that take in vectors and output real (or complex) numbers, and $(1,0)$-Tensors are cocovectors, linear functions that take in covectors and output real (or complex) numbers. Furthermore, each $(1,0)$-Tensor has a unique description as a vector, particularly the evaluation function at a particular vector. When we are being lax with these distinctions (as we often do), we can say that rank-1 Tensors are functions, and $(0,1)$-Tensors are covectors and $(1,0)$-Tensors "are" vectors. The emphasis, however, should be on the fact that tensors are functions. This is the big idea of this whole thing. Explicitly:
    </p>
  <div class="thm"><b>Definition:</b> Let $V$ be a vector space over a field $F$. Then
    <ul>
      <li> A <b>$(0,1)$-Tensor</b> is a linear function $T:V\rightarrow F$. That is, a covector.</li>
      <li> A <b>$(1,0)$-Tensor</b> is a linear function $T:V^* \rightarrow F$ and can be naturally identified with a vector in $V$.</li>
    </ul>
      </div>
    <h3>Tensors</h3>
    <p>
      An arbitrary tensor is a generalization of a covector, and other rank-1 tensors, to multivariable functions. In this context, "multivariable" means that we input multiple vectors/covectors. In this way, a $(k,\ell)$-Tensor will be some kind of function that inputs $\ell$ vectors and $k$ covectors and outputs real (or complex) numbers. The difficulty is then figuring out how to generalize the linearity relations to multiple inputs. The simplest way to achieve this is to say that they satisfy these equations individually in each variable separately. Such functions are called "Multilinear", and so a $(k,\ell)$-Tensor is a multilinear function  that inputs $\ell$ vectors and $k$ covectors and outputs a real (or complex) number.
    </p>
    <p>
      This is best illustrated in the two variable case. A $(1,1)$-Tensor is a multilinear function $T(\alpha,v)$ that inputs a single vector $v$ and covector $\alpha$ and outputs a real (or complex) number. The multilinearity means that it must satisfy the equations
      $$
      \begin{eqnarray*}
      T(\alpha,v+w) &=& T(\alpha,v) + T(\alpha,w)\\
      T(\alpha,r\cdot v) &=& r\cdot T(\alpha,v)\\
      T(\alpha+\beta,v) &=& T(\alpha,v) + T(\beta,v) \\
      T(r\cdot\alpha,v) &=& r\cdot T(\alpha,v)
      \end{eqnarray*}
      $$
      for vectors $v,w$ in $V$, covectors $\alpha,\beta$ in $V^*$ and scalar $r$. This just means that it is linear in each argument separately. A simple example of a $(1,1)$-tensor is the "Evaluation Tensor", $E(\alpha,v)$ defined as
      $$
      E(\alpha,v) = \alpha(v)
      $$
   You input a vector, and covector and output whatever value you get when you apply the input vector to the input covector. In fact, if $L$ is any linear function from $V$ into $V$, which means that it inputs vectors in $V$, outputs vectors in $V$, and obeys a linearity relation, then we can  create the $(1,1)$-tensor $T_L$ defined as
      $$
      T_L(\alpha, v) = \alpha(Tv)
      $$
      First apply $T$ to $v$ to get a new vector in $V$, then apply $\alpha$ to the result. What is not-at-all obvious is that every $(1,1)$-tensor can be written in this way. That is, every linear transformation can be interpreted uniquely as a $(1,1)$-tensor, and every $(1,1)$-tensor can be interpreted as a linear transformation. These are, a prior, different objects that can be interpreted as each other. This does not mean that linear transformations are $(1,1)$-tensors, just that we have this interpretation. This is similar to how vectors can be interpreted as $(1,0)$-tensors. The thing that tensors generalize are covectors, and through this generalization, we find that we can interpret other, unrelated objects, in this context. This is helpful, but makes the description "Tensors generalize vectors and matrices" misleading, because we're not generalizing the vector-ness of a vector, or the linear transformation-ness of a matix, we're generalizing them only once we throw them into a totally different context. Again, this description of a tensor should really just be "Tensors generalize covectors", since we are generalizing the function-ness inherent to covectors, something that vectors are not assumed to have.
    </p>
    <p>
      Tensors of rank other than $(1,0)$, $(0,1)$ or $(1,1)$ do not have nice interpretations as basic linear algebra objects. With some extra tools, we might be able to interpret $(2,0)$ and $(0,2)$ tensors are matrices, but this would not be illustrative. The takeaway of all this should be that tensor are multivariable functions that input vectors and covectors and output real (or complex) numbers and are linear. In general, a $(k,\ell)$-Tensor $T$ takes in vectors $v_1,...,v_{\ell}$ from $V$ and covectors $\alpha_1,...,\alpha_k$ from $V^*$ and outputs a number denoted
      $$
      T(\alpha_1,...,\alpha_k;v_1,...,v_{\ell})
      $$
      It cannot be overemphsized that we got this by trying to generalize the notion of a covector, particularly how they behave as functions, not from generalizing scalars, vectors or matrices. 
    </p>
    <h3>Applications 1</h3>
    <p> Since most people looking at this will probably be non-mathematicians, your head is probably hurting from all this abstract talk, and you really want some application to get a feel for all this talk. I'll then reward you with what a mathematician would consider an "application". </p>
    <h4>The Cauchy-Stress Tensor</h4>
    <p>
      

  </div>



</body>
</html>
