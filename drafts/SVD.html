<html>
<head>

<script type="text/x-mathjax-config">
  MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}});
</script>
<script type="text/javascript" async
  src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_CHTML">
</script>

<link rel="stylesheet" type="text/css" href="style.css">



<title>Singular Value Decomposition</title>
</head>
<body>
  <div class="title">
    <h1 id="titleheading">Local-to-Global</h1>
  </div>
  <hr id="titlebreak">
  
  <h2 id = "titleheading">Singular Value Decomposition</h2>
  <div class="post-content">
  <div class="post-date">11/21/2016</div>
  <p>
  Singular Value Decomposition (SVD) is a great way to decompose a linear transformation with tons of applications. Generally, to get some kind of intuition about how the decomposition works and what each term means, SVD is often explained through nice visuals of how each term behaves in simple terms. This is good if you simply want to apply SVD to a problem, but how should we think of it mathematically?
  </p>
  <p>
  SVD can be much more interesting when viewed through the lense of abstract linear algebra. Fromm this point of view, it is the consequence of three very important concepts: 
  <ul>
  <li>Linear Transformations and Invariant Subspaces</li>
   <li>Inner Products and Dual Spaces</li>
   <li>The Spectral Theorem</li>
   <li>Bases and the Standard SVD</li>
   </ul>
   Let's look at each of these ingrediants and see how each provides new insight into the abstract behaviour of SVD.
   </p>
   <h3>Linear Transformations and Invariant Subspaces</h3>
   <p>
   Let $V$ be a vector space over some field, and $T:V\rightarrow V$ a linear transformation on $V$. Such a map, from a space into itself, is an endomorphism. In general, $T$ can be difficult to understand, especially if the dimension of $V$ is large. The idea of Invariant Subspaces is to find smaller subspaces of $V$ that we can use to understand a bit of $T$. 
   </p>
   <p>
   An <b>Invariant Subspace</b> of $T$ is a nontrivial subspace $W\subset V$ so that $T(W)\subseteq W$. That is, $T$ sends every element of $W$ back into $W$. This means that we can restrict $T$ to a smaller subspace and still have an endomorphism. This means that the function $T$ behaves nicely with the inclusion $W\subset V$.
   </p>
   <p>As an example, if $T$ is a 3D rotation, then it has two invariant subspaces: The axis of the rotation, and the plane of rotation. $T$ doesn't do anything to the elements on the axis of rotation, and $T$ reduces to a 2D rotation on the plane of rotation. So to understand a 3D rotation, we just need to know how to find the plane of rotation, and then understand 2D rotations.
   </p>
   <p>
   As with the above example, it may be possible to find two invariant subspaces $W_1,W_2$ of the map $T$ so that $V=W_1\oplus W_2$. If $T_1,T_2$ are the maps when we restrict $T$ to $W_1$ and $W_2$, then we say that we can decompose $T$ as
   $$
   T = T_1\oplus T_2
   $$
   To figure out how $T$ behaves on $v\in V$, we can just write $v=w_1+w_2$, with $w_i\in W_i$, and we have
   $$
   T(v) = T_1(w_1)+T_2(w_2)
   $$
   So understanding $T$ reduces to understanding $T_1$ and $T_2$. In general, if $T:V\rightarrow V$ is any linear transformation, then there are unique invariant subspaces $V_1,...,V_n$ (up to reorder) so that 
   $$
   V=K\oplus V_1\oplus V_2\oplus\cdots\oplus V_n
   $$
   where $K$ is the kernel, and $T$ decomposes as 
   $$
   T=0\oplus T_1\oplus T_2\oplus\cdots \oplus T_n
   $$
   and, furthermore, we cannot further decompose any of the maps $T_i$. This is the <b>Invariant Decomposition</b> of $T$. This allows us to write $T$ as a block-diagonal matrix, each block corresponding to a $T_i$. An important case of this is when each $W_i$ has dimension 1, in which case this decomposition is identical to the eigenvalue decomposition of $T$, and the block diagonal matrix is just the diagonalization of $T$.
   </p>
   <p>
   Now we can see what the invariant decomposition of a map says about SVD. In this case, we have two vector spaces $U,V$, and two maps:
   $$
   \begin{eqnarray*}
   &&T:U\rightarrow V\\
   &&S:V\rightarrow U
   \end{eqnarray*}
   $$
   We can compose these in two different ways to get two different endomorphisms:
   $$
   \begin{eqnarray*}
   && ST : U\rightarrow U\\
   && TS : V\rightarrow V
   \end{eqnarray*}
   $$
   Both of these maps have their own invariant decomposition, and Singular Value Decomposition says that these two invariant decompositions are <i>compatible</i>. This can be seen as a Proto-SVD. Explicitly, if 
   $$
   \begin{eqnarray*}
   U &=& U_0\oplus U_1\oplus\cdots\oplus U_m\\
   V &=& V_0\oplus V_1\oplus\cdots\oplus V_n
   \end{eqnarray*}
   $$
   are the invariant decompositions of $ST$ and $TS$, where $U_0$ and $V_0$ are the kernels, then $m=n$ and we can order them so that for $i>0$ we have
   $$
   \begin{eqnarray*}
   T(U_i) &=& V_i\\
   S(V_i) &= & U_i
   \end{eqnarray*}
   $$
     And $T(U_0)\subseteq V_0$ and $S(V_0)\subseteq U_0$. In a way, then, this extends the notion of "Invariant Subspace" to non-endomorphisms. We can look at this a bit more functionally as well. If $T_i$ is $T$ restricted to $U_i$, $S_i$ is $S$ restricted to $V_i$, $(ST)_i$ is $ST$ restricted to $U_i$ and $(TS)_i$ is $TS$ restricted to $V_i$, then we have the functional equations:
   $$
   \begin{eqnarray*}
   (ST)_i &=& S_iT_i\\
   (TS)_i &=& T_i S_i
   \end{eqnarray*}
   $$
   Which means that restriction behaves well with composition. This is, really, the fundamental behaviour of SVD. The rest of the statements are then just this applied to particular cases with some added structure.

<h3>Inner Products and Dual Spaces</h3>
<p>
If $V$ is a vector space over the field $F$, then $V^*$ denotes its "Dual Space". This is the set of all linear functions of the form $\phi:V\rightarrow F$. If $T:V\rightarrow W$ is a linear map, then this induces a function $T^*:W^*\rightarrow V^*$ defined as follows: If $\phi:W\rightarrow F$, then $T^*(\phi)$ will have to be a function $V\rightarrow F$, it is defined as
$$
T^*(\phi)(v) := \phi(T(v))
$$
We essentially precompose the functions in $W^*$ with $T$ to get a function in $V^*$. The function $T^*$ is called the "Adjoint Map" of $T$. This process says that we can view the relationship between $V$ and $W$ determined by $T:V\rightarrow W$ in reverse between $V^*$ and $W^*$ via $T^*:W^*\rightarrow V^*$. 
</p>
<p>
In finite dimensions, $V$ and $V^*$ are isomorphic, but generally not in a meaningful way that is useful. More of as a matter of convenience. However, there is a way to see them as meaningfully isomorphic. But we'll need the extra ingredient of an Inner Product. In the most standard of situation, $V=\mathbb{R}^n$, we can view the Dot Product as a function $\cdot:V\times V\rightarrow \mathbb{R}$ that satisfies certain properties. These properties are:
<ul>
<li>For all $v\in V$ we have $v\cdot v \geq 0$, and only equal to zero if $v=0$</li>
<li>For all $v_0\in V$, the function $v \mapsto v_0\cdot v$ is linear</li>
<li>For all $v_1,v_2\in V$ we have $v_1\cdot v_2 = v_2\cdot v_1$ </li>
</ul>
These are all the properties that make dot products useful. An Inner Product is a function on a real or complex vector space that obeys these rules (the only caveat is that in the complex case, switching the order means you need to complex conjugate). 
</p>
<p>
If $\langle,\rangle$ is an Inner Product on $V$ and $v_0\in V$, then the second properties implies that the function $\phi_{v_0}$ defined by
$$
\phi_{v_0}(v) = \langle v_0,v\rangle
$$
is a linear function from $V$ into the base field. In other words, $\phi_{v_0}\in V^*$. This means actually defines a map from $V\rightarrow V^*$, by sending $v_0$ to $\phi_{v_0}$ that is always an isomorphism for finite dimensional $V$. So inner products mean that $V$ and $V^*$ are meaningfully isomorphic.
</p>
<p>
If $V$ and $W$ both have inner products, we have a map $T:V\rightarrow W$ and we identify $V=V^*$ and $W=W^*$ through the above isomorphism, then we get a map $T^*:W\rightarrow V$. When we are identifying spaces with their dual, we'll write $T^*=T^T$ and call $T^T$ the "Transpose" of $T$. This is the unique map so that
$$
\langle w, T(v)\rangle_W = \langle T^T(w),v\rangle_V
$$
for all $v\in V$ and $w\in W$. If we represent $T$ as a matrix, then the matrix for $T^T$ is the transpose of that matrix. 
</p>
<p>
We can now think of SVD. Before, we were able to say something about two maps going between the same two spaces in opposite directions. Now, if $V$ and $W$ are two inner product spaces and we have a map $T:V\rightarrow W$, then we get a reverse map for free, $T^T:W\rightarrow V$. So from a single map, we can get an SVD statement. Namely, if $T:V\rightarrow W$ is a map between two inner product spaces, then we can decompose $V$ and $W$ as
$$
\begin{eqnarray*}
V &=& V_0\oplus V_1\oplus\cdots\oplus V_n\\
W &=& W_0\oplus W_1\oplus \cdots \oplus W_n
\end{eqnarray*}
$$
so that the $V_i$ is the invariant decomposition of $V$ for the map $T^TT$ and the $W_i$ is the invariant decomposition for the map $TT^T$ and, furthermore $T(V_0)\subseteq W_0$ and $T^T(W_0)\subseteq V_0$ and for $i>0$ we have
$$
\begin{eqnarray*}
T(V_i)&= & W_i\\
T^T(W_i)&= & V_i
\end{eqnarray*}
$$
This one then tells us information strictly about $T$, not how it might interact with another map $S$. 
  </p>
  <h3>The Spectral Theorem</h3>
    <p>
      The Spectral Theorem is an important theorem about certain kinds of linear transformations on inner product spaces. In particular, it tells us exactly how the invariant decompositions work for these maps. The maps of interest are "Hermitian" or "Symmetric" maps. These are linear transformations $L:V\rightarrow V$, where $V$ is an inner product space, so that
      $$
      L^T=L
      $$
      Which means that $L$ is its own transpose/adjoint. These are the maps that satisfy $\langle Lv_1,v_2\rangle = \langle v_1,Lv_2\rangle$ for the inner product on $V$. These maps are of interest to us because both $T^TT$ and $TT^T$ are Hermitian.
    </p>
    <p>
      Explicitly, the Spectral Theorem says that if $L:V\rightarrow V$ is a Hermitian map and if $V=V_0\oplus V_1\oplus\cdots\oplus V_n$ is the invariant decomposition of $L$, then:
      <ul>
        <li> For $i>0$, the spaces $V_i$ are one dimensionsal</li>
        <li> For each $i>0$ there is a scalar $\lambda_i$ so that if $v_i\in V_i$, then $L(v_i)=\lambda_i v_i$, when $V$ is a complex vector space, the $\lambda_i$ are real</li>
        <li> If $v_i\in V_i$ and $v_j\in V_j$ and $i\ne j$ then $\langle v_i,v_j\rangle =0$</li>
        </ul>
  This allows us to refine what we learned in the previous section. If $T:V\rightarrow W$ is a linear map between inner product spaces, then the maps $T^TT:V\rightarrow V$ and $TT^T:W\rightarrow W$ are Hermitian. The Spectral Theorem then says that we can decompose the spaces into kernel+one dimensional subspaces:
  $$
  \begin{eqnarray*}
  V &=& V_0\oplus V_1\oplus\cdots \oplus V_n\\
  W &=& W_0\oplus W_1 \oplus\cdots \oplus W_n
  \end{eqnarray*}
  $$
  Furthermore, for $i>0$ there is a $\lambda_i\ne 0$ so that for $v_i\in V_i$ we have $T^TT(v_i)=\lambda_i v_i$ and there is a $\delta_i\ne 0$ so that for $w_i\in W_i$ we have $TT^T(w_i)=\delta_i w_i$. 
  </p>
<p>
  The Spectral Theorem does a little bit more for us. Since each of these subspaces are 1-dimensional and since we have an inner product, we are in the position to talk about a natural basis. This is a basis chosen for us by the theory, rather than us making choices to find one. Any single vector in $V_i$ or $W_i$ for $i>0$ will form a basis for that particular space. So if we choose a single $v_i\in V_i$ and $w_i\in W_i$ for $i>0$, then the sets $\{v_1,...,v_n\}$ and $\{w_1,...,w_n\}$ form bases for the nonkernel subspaces:
  $$
  \begin{eqnarray*}
  V_1\oplus\cdots \oplus V_n &\subseteq& V\\
  W_1\oplus\cdots \oplus W_n &\subseteq& W
  \end{eqnarray*}
  $$
  Furthermore, by the Spectral Theorem, this is an orthogonal basis. Since we have an inner product, we can choose these elements so that $\langle v_i,v_i\rangle =1$ and $\langle w_i,w_i\rangle =1$, so that we get an orthonormal basis. This basis will allow us to investigate $T$ and $T^T$ a bit more.
</p>
<p>
  Since $T(V_i)=W_i$, it follows that there is some nonzero number $a_i$ so that $T(v_i)=a_iw_i$ (where the $v_i$ and $w_i$ are our orthonormal bases). Conversely, since $T^T(W_i)=V_i$, there is some nonzero number $a^*_i$ so that $T^T(w_i)=a^*_iv_i$. These $a_i$ and $a^*_i$ are the Singular Values for $T$, and standard results about inner products show that $a^*_i$ is the complex conjugate of $a_i$ (so equal to $a_i$ if the vector space is real). Furthermore, since we can relate these to the eigenvalues for $T^TT$ and $TT^T$ by applying them in succession. From this, we get that 
  $$
  \begin{eqnarray*}
  \lambda_i &=& a_ia^*_i\\
  \delta_i &=& a^*_ia_i
  \end{eqnarray*}
  $$
  In particular, $\lambda_i=\delta_i$, and $\lambda_i=|a_i|^2$. Since these are all positive, we say that $T^TT$ and $TT^T$ are positive Hermitian map.</p>
<p>
  We can extend this discussion to the whole spaces $V$ and $W$ by noting that $ker(T^TT)=ker(T)$. This is thanks to the Fundamental Theorem of Linear Algebra. So if we choose any orthonormal basis for $V_0$, then $T$ acts on this basis by multiplication by zero. Similarly or $W_0$. So if $v_{-m},...,v_0$ is an orthonormal basis for $V_0$, then $v_{-m},...,v_0,v_1,...,v_n$ is an orthonormal basis for $V$, and for each $i\geq -m$ there is a number $a_i$ so that 
  $$
  T(v_i) = a_i
  $$
  and if $i\leq 0$ then $a_i=0$. Similarly for $T^T$. This provides a complete description of the map $T$. 
  </p>
<h3>Bases and the Standard SVD</h3>
<p>
  Now that we have explored all the theory and know exactly how our maps segment our vector spaces, we can talk about the case when we begin with an arbitrary basis. In this case, we can assume $V=K^n$ and $W=K^m$, where $K=\mathbb{R},\mathbb{C}$. With these bases, we can represent $T$ by an $m\times n$ matrix $M_T$. SVD then says that there exist unitary matrices $A$ and $B$, so that $A$ is an $n\times n$ matrix and $B$ is an $m\times m$ matrix, and an $m\times n$ diagonal matrix $D$ so that
  $$
  M_T = B D A
  $$
  The fact that $A$ and $B$ are unitary means that $A^T=A^{-1}$ and $B^T=B^{-1}$. The reason for this kind of decomposition is all of the theoretical descriptions and decompositions of the previous sections. We can read this as follows:
  <ul>
    <li> The matrix $A$ takes the basis we begin with in $V$ to our orthonormal eigendecomposition given by the Spectral Theorem. The fact that it is unitary follows from the fact that it transforms into an orthonormal basis. </li>
    <li> The matrix $D$ is the action of $T$ given directly by the invariant decomposition, which the Spectral Theorem says is given by 1-dimensional subspaces and so must be diagonal</li>
    <li> The matrix $B$ then takes our orthonormal basis in $W$ to the basis we begin with. It is unitary for the same reasons as the matri $A$.</li>
</ul>
The entries in $D$ are the Singular Values for the transformation $T$. If you take their norm, you then recover the eigenvalues for $T^TT$. If you transpose the entire SVD equation, you get the identical statement but for the map $T^T$ instead.
</p>
<p>
So this decomposition is actually a compact way to write the above story. But this is illuminating only if you know what the story is in the first place. In fact, pretty much any matrix decomposition theorem can be interpreted at a higher level in terms of decompositions and actions of linear transformations.
</p>
          
  </div>



</body>
</html>
