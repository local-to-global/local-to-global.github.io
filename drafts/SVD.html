<html>
<head>

<script type="text/x-mathjax-config">
  MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}});
</script>
<script type="text/javascript" async
  src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_CHTML">
</script>

<link rel="stylesheet" type="text/css" href="style.css">



<title>Singular Value Decomposition</title>
</head>
<body>
  <div class="title">
    <h1 id="titleheading">Local-to-Global</h1>
  </div>
  <hr id="titlebreak">
  
  <h2 id = "titleheading">Singular Value Decomposition</h2>
  <div class="post-content">
  <div class="post-date">11/21/2016</div>
  <p>
  Singular Value Decomposition (SVD) is a great way to decompose a linear transformation with tons of applications. Generally, to get some kind of intuition about how the decomposition works and what each term means, SVD is often explained through nice visuals of how each term behaves in simple terms. This is good if you simply want to apply SVD to a problem, but how should we think of it mathematically?
  </p>
  <p>
  SVD can be much more interesting when viewed through the lense of abstract linear algebra. Fromm this point of view, it is the consequence of three very important concepts: 
  <ul>
  <li>Linear Transformations and Invariant Subspaces</li>
   <li>Inner Products and Dual Spaces</li>
   <li>The Spectral Theorem</li>
   <li>Bases and the Standard SVD</li>
   </ul>
   Let's look at each of these ingrediants and see how each provides new insight into the abstract behaviour of SVD.
   </p>
   <h3>Linear Transformations and Invariant Subspaces</h3>
   <p>
   Let $V$ be a vector space over some field, and $T:V\rightarrow V$ a linear transformation on $V$. Such a map, from a space into itself, is an endomorphism. In general, $T$ can be difficult to understand, especially if the dimension of $V$ is large. The idea of Invariant Subspaces is to find smaller subspaces of $V$ that we can use to understand a bit of $T$. 
   </p>
   <p>
   An <b>Invariant Subspace</b> of $T$ is a nontrivial subspace $W\subset V$ so that $T(W)\subseteq W$. That is, $T$ sends every element of $W$ back into $W$. This means that we can restrict $T$ to a smaller subspace and still have an endomorphism. This means that the function $T$ behaves nicely with the inclusion $W\subset V$.
   </p>
   <p>As an example, if $T$ is a 3D rotation, then it has two invariant subspaces: The axis of the rotation, and the plane of rotation. $T$ doesn't do anything to the elements on the axis of rotation, and $T$ reduces to a 2D rotation on the plane of rotation. So to understand a 3D rotation, we just need to know how to find the plane of rotation, and then understand 2D rotations.
   </p>
   <p>
   As with the above example, it may be possible to find two invariant subspaces $W_1,W_2$ of the map $T$ so that $V=W_1\oplus W_2$. If $T_1,T_2$ are the maps when we restrict $T$ to $W_1$ and $W_2$, then we say that we can decompose $T$ as
   $$
   T = T_1\oplus T_2
   $$
   To figure out how $T$ behaves on $v\in V$, we can just write $v=w_1+w_2$, with $w_i\in W_i$, and we have
   $$
   T(v) = T_1(w_1)+T_2(w_2)
   $$
   So understanding $T$ reduces to understanding $T_1$ and $T_2$. In general, if $T:V\rightarrow V$ is any linear transformation, then there are unique invariant subspaces $W_1,...,W_n$ (up to reorder) so that 
   $$
   V=W_1\oplus W_2\oplus\cdots\oplus W_n
   $$
   and $T$ decomposes as 
   $$
   T=T_1\oplus T_2\oplus\cdots \oplus T_n
   $$
   and, furthermore, we cannot further decompose any of the maps $T_i$. This is the <b>Invariant Decomposition</b> of $T$. This allows us to write $T$ as a block-diagonal matrix, each block corresponding to a $T_i$. An important case of this is when each $W_i$ has dimension 1, in which case this decomposition is identical to the eigenvalue decomposition of $T$, and the block diagonal matrix is just the diagonalization of $T$.
   </p>
   <p>
   Now we can see what the invariant decomposition of a map says about SVD. In this case, we have two vector spaces $U,V$, and two maps:
   $$
   \begin{eqnarray*}
   &&T:U\rightarrow V\\
   &&S:V\rightarrow U
   \end{eqnarray*}
   $$
   We can compose these in two different ways to get two different endomorphisms:
   $$
   \begin{eqnarray*}
   && ST : U\rightarrow U\\
   && TS : V\rightarrow V
   \end{eqnarray*}
   $$
   Both of these maps have their own invariant decomposition, and Singular Value Decomposition says that these two invariant decompositions are <i>compatible</i>. This can be seen as a Proto-SVD. Explicitly, if 
   $$
   \begin{eqnarray*}
   U &=& U_1\oplus\cdots\oplus U_m\\
   V &=& V_1\oplus\cdots\oplus V_n
   \end{eqnarray*}
   $$
   are the invariant decompositions of $ST$ and $TS$, then $m=n$ and we can order them so that
   $$
   \begin{eqnarray*}
   T(U_i) &\subseteq & V_i\\
   S(V_i) &\subseteq & U_i
   \end{eqnarray*}
   $$
   In a way, then, this extends the notion of "Invariant Subspace" to non-endomorphisms. We can look at this a bit more functionally as well. If $T_i$ is $T$ restricted to $U_i$, $S_i$ is $S$ restricted to $V_i$, $(ST)_i$ is $ST$ restricted to $U_i$ and $(TS)_i$ is $TS$ restricted to $V_i$, then we have the functional equations:
   $$
   \begin{eqnarray*}
   (ST)_i &=& S_iT_i\\
   (TS)_i &=& T_i S_i
   \end{eqnarray*}
   $$
   Which means that restriction behaves well with composition. This is, really, the fundamental behaviour of SVD. The rest of the statements are then just this applied to particular cases with some added structure.

<h3>Inner Products and Dual Spaces</h3>
<p>
If $V$ is a vector space over the field $F$, then $V^*$ denotes its "Dual Space". This is the set of all linear functions of the form $\phi:V\rightarrow F$. If $T:V\rightarrow W$ is a linear map, then this induces a function $T^*:W^*\rightarrow V^*$ defined as follows: If $\phi:W\rightarrow F$, then $T^*(\phi)$ will have to be a function $V\rightarrow F$, it is defined as
$$
T^*(\phi)(v) := \phi(T(v))
$$
We essentially precompose the functions in $W^*$ with $T$ to get a function in $V^*$. The function $T^*$ is called the "Adjoint Map" of $T$. This process says that we can view the relationship between $V$ and $W$ determined by $T:V\rightarrow W$ in reverse between $V^*$ and $W^*$ via $T^*:W^*\rightarrow V^*$. 
</p>
<p>
In finite dimensions, $V$ and $V^*$ are isomorphic, but generally not in a meaningful way that is useful. More of as a matter of convenience. However, there is a way to see them as meaningfully isomorphic. But we'll need the extra ingredient of an Inner Product. In the most standard of situation, $V=\mathbb{R}^n$, we can view the Dot Product as a function $\cdot:V\times V\rightarrow \mathbb{R}$ that satisfies certain properties. These properties are:
<ul>
<li>For all $v\in V$ we have $v\cdot v \geq 0$, and only equal to zero if $v=0$</li>
<li>For all $v_0\in V$, the function $v \mapsto v_0\cdot v$ is linear</li>
<li>For all $v_1,v_2\in V$ we have $v_1\cdot v_2 = v_2\cdot v_1$ </li>
</ul>
These are all the properties that make dot products useful. An Inner Product is a function on a real or complex vector space that obeys these rules (the only caveat is that in the complex case, switching the order means you need to complex conjugate). 
</p>
<p>
If $\langle,\rangle$ is an Inner Product on $V$ and $v_0\in V$, then the second properties implies that the function $\phi_{v_0}$ defined by
$$
\phi_{v_0}(v) = \langle v_0,v\rangle
$$
is a linear function from $V$ into the base field. In other words, $\phi_{v_0}\in V^*$. This means actually defines a map from $V\rightarrow V^*$, by sending $v_0$ to $\phi_{v_0}$ that is always an isomorphism for finite dimensional $V$. So inner products mean that $V$ and $V^*$ are meaningfully isomorphic.
</p>
<p>
If $V$ and $W$ both have inner products, we have a map $T:V\rightarrow W$ and we identify $V=V^*$ and $W=W^*$ through the above isomorphism, then we get a map $T^*:W\rightarrow V$. When we are identifying spaces with their dual, we'll write $T^*=T^T$ and call $T^T$ the "Transpose" of $T$. This is the unique map so that
$$
\langle w, T(v)\rangle_W = \langle T^T(w),v\rangle_V
$$
for all $v\in V$ and $w\in W$. If we represent $T$ as a matrix, then the matrix for $T^T$ is the transpose of that matrix. 
</p>
<p>
We can now think of SVD. Before, we were able to say something about two maps going between the same two spaces in opposite directions. Now, if $V$ and $W$ are two inner product spaces and we have a map $T:V\rightarrow W$, then we get a reverse map for free, $T^T:W\rightarrow V$. So from a single map, we can get an SVD statement. Namely, if $T:V\rightarrow W$ is a map between two inner product spaces, then we can decompose $V$ and $W$ as
$$
\begin{eqnarray*}
V &=& V_1\oplus\cdots\oplus V_n\\
W &=& W_1\oplus \cdots \oplus W_n
\end{eqnarray*}
$$
so that the $V_i$ is the invariant decomposition of $V$ for the map $T^TT$ and the $W_i$ is the invariant decomposition for the map $TT^T$ and, furthermore
$$
\begin{eqnarray*}
T(V_i)&\subseteq & W_i\\
T^T(W_i)&\subseteq & V_i
\end{eqnarray*}
$$
This one then tells us information strictly about $T$, not how it might interact with another map $S$. 
  </p>
  <h3>The Spectral Theorem</h3>
    <p>
      The Spectral Theorem is an important theorem about certain kinds of linear transformations on inner product spaces. In particular, it tells us exactly how the invariant decompositions work for these maps. The maps of interest are "Hermitian" or "Symmetric" maps. These are linear transformations $L:V\rightarrow V$, where $V$ is an inner product space, so that
      $$
      L^T=L
      $$
      Which means that $L$ is its own transpose/adjoint. These are the maps that satisfy $\langle Lv_1,v_2\rangle = \langle v_1,Lv_2\rangle$ for the inner product on $V$. These maps are of interest to us because both $T^TT$ and $TT^T$ are Hermitian.
    </p>
    <p>
      Explicitly, the Spectral Theorem says that if $L:V\rightarrow V$ is a Hermitian map and if $V=V_1\oplus\cdots\oplus V_n$ is the invariant decomposition of $L$, then:
      <ul>
        <li> Each of the spaces $V_i$ is one dimensionsal</li>
        <li> For each $i$ there is a scalar $\lambda_i$ so that if $v_i\in V_i$, then $L(v_i)=\lambda_i v_i$, when $V$ is a complex vector space, the $\lambda_i$ are real</li>
        <li> If $v_i\in V_i$ and $v_j\in V_j$ and $i\ne j$ then $\langle v_i,v_j\rangle =0$</li>
        </ul>
          
  </div>



</body>
</html>
