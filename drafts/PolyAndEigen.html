<html>
<head>

<script type="text/x-mathjax-config">
  MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}});
</script>
<script type="text/javascript" async
  src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_CHTML">
</script>

<link rel="stylesheet" type="text/css" href="style.css">



<title>Polynomials and Eigenspaces</title>
</head>
<body>
  <div class="title">
    <h1 id="titleheading">Polynomials, Eigenspaces and Jordan Blocks</h1>
  </div>
  <hr id="titlebreak">

  <div class="post-content">
  <div class="post-date">11/21/2016</div>
    <p>The behavior of eigenvalues, eigenspaces and eigendecompositions are heavily influenced by the arithmetic of polynomials. This will  just be an overview of how this works and the proofs will be left to the reader.
    </p>
    <p> The key thing about this is that if $V$ is a vector space over some field then a linear transformation $T:V\rightarrow V$, that we might want to understand, can turn a polynomial $p(x)$ into a linear transformation $p(T)$. Explicitly, if $p(x)=a_nx^n+\cdots+a_1x+a_0$ then $$p(T):=a_nT^n+\cdots+a_1T+a_0I$$ where $I$ is the identity linear transformation. This means, given a single $T$, we can imprint the behavior of any polynomial onto our vector space and how this imprinting works can lead to things like Eigenspaces and Eigendecomposition.
    </p>
    <h3>Eigenspaces</h3>
    <p>Recall that if $F:V\rightarrow V$ is any linear transformation, then $F$ determines a fundamental subspace of $T$, the kernel $$\ker(F)=\{v\in V\;|\; F(v)=0\}$$ The linear transformation $T$ has one itself, but so do every polynomial combination of $T$, $p(T)$. Call $E_p=\ker(p(T))$ the $p$-Eigenspace of $T$. There are a few great things about these eigenspaces. One of the most important of which is that they are all invariante subspaces of $T$. This means that if $v\in E_p$ then so is $Tv$. Finding invariant subspaces, and things like them, is a process that is interesting in all parts of math. In Linear Algebra, invariant subspaces help simplify questions about the associated linear transformation by allowing us to look at smaller subspaces or subspaces with specific structure. The fact that we can explicitly describe certain invariant subspaces as these eigenspaces is powerful.
    </p>
    <p>Note that this notion of eigenspace corresponds with the tranditional notion of an eigenspace. In particular, if $\lambda$ is an eigenvalue, then we can put $p(x)=x-\lambda$ and we'll find that the $\lambda$-eigenspace, in the traditional sense, is the same as the $p(x)$-eigenspace in this sense. The advantage of this is that we can now directly talk about how polynomials interact with these things.
    </p>
    <p>
      We can also import some of the polynomial arithmetic to help decompose our vector space. The key thing is that if $p(x)$ and $q(x)$ are polynomials with greatest common divisor $g(x)$, then we have
      $$
      E_p \cap E_q = E_g
      $$
      If we think of the greatest common divisor as how much two polynomials (or numbers) multiplicatively overlap, then this formual tells us that this transfers directly to how much the associated subspaces overlap. Related, we also have that 
      $$
      E_p+E_q \subseteq E_{pq}
      $$
      where the sum is not necessarily direct. When $p(x)$ and $q(x)$ are coprime, it follows that $E_p\cap E_q = 0$ and this turns into the traditional statement that eigenspaces with different eigenvalues do not intersect. In addition, when $p(x)$ and $q(x)$ are coprime, the sum $E_p+E_q$ is direct and, in addition, we get
      $$
      E_{pq} = E_p+E_q
      $$
      This is almost the complete statement of the Eigendecomposition of a linear transformation. In particular, if we can write $p(x)=p_1(x)^{k_1}\cdots p_n^{k_n}(x)$, the irreducible decomposition of a polynomial, then we can write $E_p$ as the direct sum
      $$
      E_p = E_{p_1^{k_1}}+\cdots +E_{p_n^{k_n}}
      $$
     This corresponds to the decomposition of $E_p$ into its generalized eigenspaces. The key thing is that we can do this to the entire vector space $V$. There exists a unique polynomial $m_t(x)$ (monic and of minimal degree) so that $E_{m_T}=V$, and applying this formula to $E_{m_T}$ results in the decomposition of the entire vector space $V$ into generalized eigenspaces. This shows that it is the arithmetic of polynomials that governs how a vector space decomposes into eigenspaces. Overall, this allows us to use linear transformations to decompose any vector space into smaller, invariant subspaces that have a particular structure. Namely, they are the eigenspaces of so-called "Primary Polynomials", which are polynomials that are a power of an irreducible polynomial. The structure of these primary eigenspaces is a little more difficult to assess.
    </p>
    <p> A subspace being an eigenspace of a primary polynomial $p^k(x)$ is the same as saying that the minimal polynomial of the original function when restricted to this subspace is $p^k(x)$.  Firstly, it may decompose further into smaller invariant vector spaces. Though if the original these are constrained to be killed by $p^{\ell}(x)$ for some $\ell\leq k$. Overall, means that we can restrict our attention to vector spaces whose minimal polynomials are primary polynomials and, moreover, they do not decompose as a direct sum of smaller invariant subspaces. These are the indecomposable components of a vector space and are what give rise to Jordan Blocks.
    </p>
    <p> It should be noted that all that has been said is nothing more than the Structure Theorem for Modules over a PID applied to the case where $T:V\rightarrow V$ turns the $K$-vector space into a $K[x]$-Module. Each of the indecomposable parts corresponds to a submodule of the form $K[x]/(p^k)$, which we will now looks at.
    <h3> Eigenspaces of Primary Polynomials</h3>
    <p> Let $V$ be a vector space and $T:V\rightarrow V$ is a linear transformation so that $V$ that cannot be broken down into a direct sum of invariant subspaces and $m_T(x)=p^k(x)$ is a primary polynomial. We can then use the linear transformation $p(T)$ to expose a structure in $V$. Note that if $i\leq k$, then we have the nonzero subspace $E_{p^i}\subseteq V$, this gives us the following flag on $V$:
      $$
      0 \subset E_p \subset E_{p^2}\subset \cdots \subset E_{p^{k-2}}\subset E_{p^{k-1}} \subset V
      $$
      Moreover, we have that $p(T):E_{p^i}\rightarrow E_{p^{i-1}}$ with kernel $E_p$ and so $p(T)$ lets us walk down this line via
      $$
      V \rightarrow E_{p^{k-1}}\rightarrow E_{p^{k-2}}\rightarrow\cdots\rightarrow E_{p^2}\rightarrow E_{p}\rightarrow 0
      $$
      leaving behind a copy of $E_p$ each time. In particular, we can find a (non-canonical) isomorphism $E_{p^i}=(E_p)^{i}$.  Note that $E_p$ is an invariant subspace of $T$ in $V$, which means that when we restrict $T$ to $E_p$, we get a function $T|_{E_p}:E_p\rightarrow E_p$. If we have a direct product $(E_p)^{i}$, then we can form a diagonalized version of this action by just haveing $T|_{E_p}$ independently act on each coordinate. Let $\Delta T:V\rightarrow V$ be the corresponding diagonal linear transformation obtained in this way from the decomposition $V=(E_p)^k$. If $p(x)=x-\lambda$, then this is just the $k\times k$ with all $\lambda$s on the diagonal, and $T$ would then be the associated $k\times k$ Jordan Block. If the degree of $p(x)$ is larger than 1, then we can only guarantee that $\Delta T$ is block-diagonal with identical blocks rather than diagonal itself.
    </p>
    <p>
      Finally, it can be shown that $N=T-\Delta T$ is a nilpotent map of degree $k$. In particular, $T-\Delta T$ sends $E_{p^i}$ to $E_{p^{i-1}}$. In this way, we can say that in such a situation, $T$ acting on a vector space $V$ that cannot be decomposed into smaller subspaces, that $T$ is "almost" equal to the block-diagonal transformation $\Delta T$ since
      $$
      T = \Delta T + N
      $$
      where $N$ is nilpotent. This is a more general version of Jordan Block Form. What is good about this is that we can describe the action of $T$, a transformation with a primary minimal polynomial, in terms of a transformation $T|_{E_p}$ that has an irreducible minimal polynomial. This means that to understand a linear transformation, all we need to do is understand how linear transformations that are indecomposable and have irreducible minimal polynomials behave. 
    <h3> Indecomposable Linear Transformations with Irredicuble Minimal Polynomial</h3>
    <p> Let $V$ be a vector space and $T:V\rightarrow V$ is a linear transformation so that $V$ that cannot be broken down into a direct sum of invariant subspaces and $m_T(x)=p(x)$ is an irreducible polynomial. Note that if $m_T(x)=x-\lambda$ is of degree 1, then $V$ must have dimension 1 and $T$ must act as $Tv=\lambda v$ and so we completely understand $T$.
    </p>
    <p>We will make the quality-of-life assumption that $p(x)$ is a separable polynomial, which means that all of its roots (in a splitting field) are unique.Let $L/K$ be a splitting field for the polynomial $p(x)$ and let $V_L=V\otimes L$ be the vector space $V$ with scalars extended to $L$. In $L$, the polynomial $p(x)$ will factor as $p(x)=(x-\lambda_1)\cdots(x-\lambda_n)$. The linear tranformation $T$ then induces a linear transformation $T_L:V_L\rightarrow V_L$. This linear transformation will have the same minimal polynomial and so $V_L$ decomposes into a direct sum of subspaces as above:
      $$
      V_L = E_1+\cdots E_n
      $$
      where $E_i$ is the $(x-\lambda_i)$-eigenspace. If $v_i\in E_i$ then $T_L$ acts as $T_Lv_i = \lambda_iv_i$. Now, $V$ lives inside $V_L$ and if we view $V_L$ as a $K$-vector space, then $V$ is an invariant subspace of $T_L$. Let $\phi:V\rightarrow V_L$ be the inclusion map (note: it is invertible when restricted to $V$). It then follows that for $v\in V$ we have 
      $$
      Tv = \phi^{-1}T_L\phi v
      $$
      In other words, there is a diagonal $L$-linear transformation $T_L$ on $V_L$ so that $T$ is similar to $T_L$. This completely classifies how linear transformations can behave.
    </p>
  </div>
  



</body>
</html>
